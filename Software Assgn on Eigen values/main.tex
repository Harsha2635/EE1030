\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{geometry}
\geometry{a4paper, margin=1in}

\title{Eigenvalue Computation Using the QR Algorithm}
\author{}
\date{}

\begin{document}

\maketitle
\section*{Introduction}
Eigenvalues are fundamental concepts in linear algebra and have applications in a wide range of fields such as physics, engineering, computer science, and data analysis. Given a square matrix \( A \), an eigenvalue \( \lambda \) is defined as a scalar such that there exists a non-zero vector \( v \) (called the eigenvector) satisfying:
\begin{equation}
    A v = \lambda v.
\end{equation}
This equation implies that multiplying the matrix \( A \) by the eigenvector \( v \) scales \( v \) by the factor \( \lambda \) without changing its direction. 

In practical terms, eigenvalues provide insights into the behavior of systems represented by matrices. For instance:
- In physics, eigenvalues describe natural frequencies of vibration or energy levels in quantum mechanics.
- In computer science, eigenvalues are used in principal component analysis (PCA) for dimensionality reduction.
- In graph theory, the eigenvalues of adjacency matrices reveal structural properties of graphs.

Computing eigenvalues is often a challenging numerical task, especially for large matrices, and is a core topic in computational linear algebra. This report focuses on computing eigenvalues using the **QR Algorithm**, a widely used iterative method.

\section*{Chosen Algorithm}
The algorithm implemented for eigenvalue computation is the **QR Algorithm**. It works iteratively by factorizing a matrix \( A \) into the product of an orthogonal matrix \( Q \) and an upper triangular matrix \( R \), then updating \( A \) as \( RQ \). Over successive iterations, the off-diagonal elements of \( A \) converge to zero, and the diagonal elements approximate the eigenvalues of the original matrix.
The key steps of the algorithm can be summarized as:
\begin{align}
    A &= QR, \quad \text{(QR decomposition of \( A \))} \\
    A' &= RQ, \quad \text{(Recompute \( A \) for the next iteration)}
\end{align}
After \( k \) iterations, the matrix \( A^{(k)} \) converges to a quasi-diagonal form:
\begin{equation}
    A^{(k)} \approx 
    \begin{bmatrix}
        \lambda_1 & \cdots & \cdots & \vdots \\
        0 & \lambda_2 & \cdots & \vdots \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots & \lambda_n
    \end{bmatrix},
\end{equation}
where \( \lambda_1, \lambda_2, \dots, \lambda_n \) are the eigenvalues of the original matrix \( A \).

\subsection*{QR Decomposition Using the Householder Algorithm}
For the QR decomposition, the **Householder Algorithm** is used. The Householder method is a numerically stable and efficient approach to compute \( Q \) and \( R \). It achieves the decomposition by iteratively applying orthogonal transformations (Householder reflections) to eliminate elements below the diagonal.

The transformation is defined as:
\begin{equation}
    H = I - 2\frac{vv^T}{v^Tv},
\end{equation}
where \( v \) is a carefully chosen vector that zeroes out all elements below the pivot in a column of \( A \).

\subsubsection*{Benefits of the Householder Algorithm}
1. Numerical Stability: Householder reflections maintain orthogonality of \( Q \) and ensure numerical stability, unlike the classical Gram-Schmidt process, which can accumulate rounding errors.\\
2. Efficiency: The algorithm is computationally efficient with a complexity of \( O(n^3) \), making it suitable for dense matrices.\\
3. Simplicity for Parallelization: The structure of Householder transformations is conducive to parallel computation, especially for large matrices.\\
4. Robustness for Complex Matrices: The method generalizes easily to complex matrices, as required in the current implementation.

\subsection*{Workflow of the Householder Algorithm}
1. Compute the Householder matrix \( H \) to zero out sub-diagonal entries in the first column.\\
2. Apply \( H \) to the matrix \( A \) to compute \( R \).\\
3. Repeat the process on the reduced submatrix for subsequent columns.

This choice of QR decomposition ensures that the algorithm performs reliably and accurately for eigenvalue computation.



\section*{Time Complexity Analysis}
The QR Algorithm has a time complexity of \( O(n^3) \) per iteration for an \( n \times n \) matrix due to matrix multiplications and factorizations. Given \( k \) iterations to achieve convergence, the overall complexity is \( O(kn^3) \). The convergence rate is typically quadratic for most matrices, although it can depend on specific matrix properties (e.g., spectral gap).

\section*{Other Insights}
\subsection*{Memory Usage}
The algorithm requires additional memory for the matrices \( Q \), \( R \), and a temporary matrix for storing intermediate results, leading to a space complexity of \( O(n^2) \).

\subsection*{Convergence Rate}
The QR Algorithm converges faster for symmetric or Hermitian matrices due to their favorable eigenvalue structure. For general matrices, the convergence rate depends on the separation between eigenvalues (the spectral gap).

\subsection*{Suitability for Different Matrices}
- Symmetric Matrices: The algorithm works efficiently and reliably.\\
- Sparse Matrices: Direct application of the QR Algorithm may be inefficient due to the loss of sparsity in intermediate steps. Specialized versions of the algorithm exist for such cases.\\
- Non-Symmetric or Complex Matrices: The implementation handles these matrices but may require more iterations to converge.

\section*{Comparison of Algorithms}
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Algorithm}        & \textbf{Time Complexity} & \textbf{Accuracy} & \textbf{Suitability}                     \\ \hline
QR Algorithm              & \( O(kn^3) \)           & High             & General-purpose; handles complex and real matrices \\ \hline
Power Iteration           & \( O(kn^2) \)           & Low (dominant eigenvalue only) & Simple; useful for large sparse matrices   \\ \hline
Jacobi Method             & \( O(n^3) \)            & High             & Best for symmetric matrices                \\ \hline
Divide-and-Conquer Method & \( O(n^3) \)            & High             & Efficient for large dense symmetric matrices \\ \hline
Arnoldi/Lanczos Method    & \( O(kn^2) \)           & High             & Optimal for sparse matrices with a few dominant eigenvalues \\ \hline
\end{tabular}

\section*{Conclusion}
The QR Algorithm is a versatile and accurate method for eigenvalue computation, particularly for dense and small-to-medium-sized matrices. However, for very large or sparse matrices, iterative methods such as Arnoldi or Lanczos are preferable.\\
This implementation also supports complex matrices, making it suitable for a broader range of applications. Convergence is controlled by a tolerance (\( 10^{-9} \)) and a maximum number of iterations (100).
\end{document}

